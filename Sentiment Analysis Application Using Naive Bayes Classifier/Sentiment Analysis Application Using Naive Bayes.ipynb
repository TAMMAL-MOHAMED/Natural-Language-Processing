{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a971c6a9-a351-45de-a79b-bcb6b988e8e4",
   "metadata": {},
   "source": [
    "# **Sentiment Analysis Application Using Naive Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b25a40b-9e90-4ee8-b8ba-714068b17753",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446bf0c4-7696-43c6-a0dd-dcb87edbc75f",
   "metadata": {},
   "source": [
    "This project is my first project in Natural Language Processing (NLP), it implements a sentiment analysis system from scratch using Python. I see this project a key application of NLP that identifies and categorizes sentiments expressed in text data, such as positive and negative reviews. <br />\n",
    "\n",
    "I will be using the Naive Bayes classifier, a probabilitic machine learning algorithm, to perform sentiment classification. The entire pipeline is built from the ground up, covering preprocessing of text data, feature extraction through vectorization, and training/testing the classifier. The implementation does not rely on external libraries for machine learning, making it an educational example of sentiment analysis fundamentals. <br />\n",
    "\n",
    "This project is valuable for understanding the inner workings of Naive Bayes, as well as essential NLP techniques such as tokenization, stopword removal, and stemming. It demonstrates the entire workflow of text-based machine learning and showcases the practicality of applying foundational concepts to real-world datasets. <br />\n",
    "\n",
    "The primary objective of this project is to classify Amazon product reviews into positive or negative sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf25431-0bc0-463e-960e-02b4ed6567a8",
   "metadata": {},
   "source": [
    "#### Import Required Libraries:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b284ebfa-7fd8-49d1-81cc-b5d6079b143c",
   "metadata": {},
   "source": [
    "Here are all libraries we need for this project. For reading and manipulating the dataset, we need **Pandas**. For regular expressions, we use **re** to clean text. We have also the **nltk** library, which provides text preprocessing tools like tokenization, stopword removal and stemming. Also, we will be calling **Numpy** library for numerical operations and efficient data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54641ed6-abf4-4fcc-9a61-076f65b4d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9c4b32-df8d-4ef3-a0d4-eff7f3279d05",
   "metadata": {},
   "source": [
    "#### Download Necessary NLTK Resources:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7df9ae9-4e6a-44a7-bb2e-87b36a32ac66",
   "metadata": {},
   "source": [
    "As mentioned before, we are in need of tokenizer for splitting sentences into words, which we get that from **punkt**, and a list of common words (like \"the,\" \"is\") to exclude from analysis, from **stopwords**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78b3f607-d22c-4ca9-b604-e57daaaf314e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54307a27-2506-42ce-91f5-3cc1378a42ef",
   "metadata": {},
   "source": [
    "####  Load and Prepare the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88abfafd-6771-436c-8809-5fe4bd4d6372",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('amazon.csv')\n",
    "\n",
    "X = df['reviewText'].fillna(\"\")  # Handle missing values by replacing them with empty strings\n",
    "Y = df['Positive']  # Labels: 1 for positive sentiment, 0 for negative sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc5476e-b485-4225-8b7a-956f43266d93",
   "metadata": {},
   "source": [
    "#### Preprocess Reviews:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c59b6a-e7c8-467b-84c4-98688aba34ff",
   "metadata": {},
   "source": [
    "In the following cell, we need to standardize text by:\n",
    "1. Remove special characters, URLs, and numbers.\n",
    "2. Tokenize sentences into words.\n",
    "3. Remove stop words (e.g., \"is\", \"the\", \"and\").\n",
    "4. Convert text to lowercase.\n",
    "5. Stem or lemmatize words (reduce words to their root form).<br />\n",
    "\n",
    "For example: <br />\n",
    "* Input: \"I absolutely love this product!\" <br />\n",
    "* Output: ['absolut', 'love', 'product']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7c2577a-6126-4139-91aa-f16f8760bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    tweet = tweet.lower()  # Convert text to lowercase\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)  # Remove punctuation\n",
    "    tokens = word_tokenize(tweet.lower())  # Split text into individual words and lowercase it\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    ps = PorterStemmer()\n",
    "    tokens = [ps.stem(word) for word in tokens]  # Reduce words to their root forms\n",
    "    return tokens\n",
    "\n",
    "preprocessed_reviews = [preprocess_tweet(review) for review in X]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d98406-60ce-486c-8725-b42a76dcda7d",
   "metadata": {},
   "source": [
    "#### Create Vocabulary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dbbe4d-071d-4bb9-bafa-b8779357ffcb",
   "metadata": {},
   "source": [
    "Then we need to build a dictionary mapping each unique word in the dataset to a unique index. <br />\n",
    "\n",
    "For example:\n",
    "* Input: ['absolut', 'love', 'product']\n",
    "* Output: {'absolut': 0, 'love': 1, 'product': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca0bedb4-52a4-433e-b2c4-6dd7c377baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(tweets):\n",
    "    vocab = {}\n",
    "    index = 0\n",
    "    for tweet in tweets:\n",
    "        for word in tweet:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = index\n",
    "                index += 1\n",
    "    return vocab\n",
    "\n",
    "vocab = create_vocabulary(preprocessed_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ce282b-7234-4550-af64-df4029fd3905",
   "metadata": {},
   "source": [
    "#### Convert Reviews into Vectors:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e26969b-8ad8-44ef-b9d8-1578a011ee7e",
   "metadata": {},
   "source": [
    "In this part, we will convert each review into a numerical vector based on word frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a462b1fb-c266-403c-b9cc-ac653b131738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_tweets(tweets, vocab):\n",
    "    vectors = []\n",
    "    for tweet in tweets:\n",
    "        vector = [0] * len(vocab)  # Initialize a vector of zeros\n",
    "        for word in tweet:\n",
    "            if word in vocab:\n",
    "                vector[vocab[word]] += 1  # Count occurrences of each word\n",
    "        vectors.append(vector)\n",
    "    return np.array(vectors)\n",
    "\n",
    "tweet_vectors = vectorize_tweets(preprocessed_reviews, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa4dca2-7cdb-4620-a83b-0840f6c47bdb",
   "metadata": {},
   "source": [
    "#### Build the Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7e0a12-5b5e-4c78-9b14-b4b7a2d37f87",
   "metadata": {},
   "source": [
    "In this project we have chosen the Naive Bayes Classifier to deal with the classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b023fb-9701-412b-bdbc-4dee2a7c476c",
   "metadata": {},
   "source": [
    "Naive Bayes Implementation (Baseline)\n",
    "A Naive Bayes classifier is a simple yet effective algorithm for text classification. We implement it as follows:\n",
    "\n",
    "**Calculate Prior Probabilities:**\n",
    "\n",
    "$P(positive)$ = $Number of positive samples \\over Total samples$\n",
    "\n",
    "Similarly, calculate $P(negative)$.\n",
    "\n",
    "**Calculate Likelihoods:** For each word in the vocabulary, calculate:\n",
    "\n",
    "$P(word∣positive)$= $Count of the word in positive samples + 1 \\over Total words in positive samples + Vocabulary size$\n",
    "\n",
    "Use Laplace smoothing to avoid zero probabilities.\n",
    "\n",
    "**Calculate Posterior Probabilities:** For a new sentence, calculate:\n",
    "\n",
    "$P(positive∣sentence)$ ∝ $P(positive)$ x $P(word1|positive)$ x $P(word2|positive)$ x $...$\n",
    "\n",
    "Classify based on the higher probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2421ef88-8704-4fb5-b96d-b221b00878d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.class_probs = {}\n",
    "        self.word_probs = {}\n",
    "\n",
    "    def train(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.class_probs = {label: np.mean(y == label) for label in np.unique(y)}  # Prior probabilities\n",
    "        self.word_probs = {}\n",
    "        for label in np.unique(y):\n",
    "            label_features = X[np.array(y) == label]\n",
    "            word_counts = np.sum(label_features, axis=0) + 1  # Laplace smoothing\n",
    "            total_count = np.sum(word_counts)\n",
    "            self.word_probs[label] = word_counts / total_count  # Likelihood\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            posteriors = {}\n",
    "            for label in self.class_probs:\n",
    "                prior = np.log(self.class_probs[label])  # Log of prior\n",
    "                likelihood = np.sum(np.log(self.word_probs[label]) * x)  # Log of likelihood\n",
    "                posteriors[label] = prior + likelihood  # Posterior\n",
    "            predictions.append(max(posteriors, key=posteriors.get))\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017fa20a-cc35-482d-ada7-af3a913525c8",
   "metadata": {},
   "source": [
    "#### Train-Test Split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c1e5ce9-fc7b-44ed-9e8e-ed613eeeaeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(tweet_vectors, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6580e5f-c9e1-432e-9032-b48ab6b7174f",
   "metadata": {},
   "source": [
    "#### Train and Evaluate the Mode:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e9acd5-26a4-4591-80f6-359cd0465aad",
   "metadata": {},
   "source": [
    "Here we will trains the Naive Bayes model on the training set, and \n",
    "evaluates accuracy on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c36d64ce-d121-4c2e-8234-7a0ef0b47fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.17%\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes()\n",
    "nb.train(X_train, np.array(Y_train))\n",
    "\n",
    "predictions = nb.predict(X_test)\n",
    "\n",
    "accuracy = np.mean(predictions == np.array(Y_test))\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d093737f-d236-404d-b8e7-78366ed81ca4",
   "metadata": {},
   "source": [
    "#### Test with New Input:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8b3bcc-be1d-42bd-9872-b738241f4b77",
   "metadata": {},
   "source": [
    "Finally, we can preprocess a new review, convert it to a vector, and predict sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "741d0053-47ce-4e17-82b8-0ad8d1c98df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment for 'This product is fantastic! I absolutely love it.': positive\n"
     ]
    }
   ],
   "source": [
    "def classify_text(text, vocab, model):\n",
    "    processed = preprocess_tweet(text)\n",
    "    vector = vectorize_tweets([processed], vocab)\n",
    "    prediction = model.predict(vector)[0]\n",
    "    return \"positive\" if prediction == 1 else \"negative\"\n",
    "\n",
    "new_review = \"This product is fantastic! I absolutely love it.\"\n",
    "print(f\"Sentiment for '{new_review}': {classify_text(new_review, vocab, nb)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b663674-4c5d-4b54-bf94-667ca7377d72",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2676025-5d64-4297-9c01-8b430e8a7f3a",
   "metadata": {},
   "source": [
    "In this project, we successfully implemented a sentiment analysis system from scratch using a Naive Bayes classifier to classify Amazon product reviews as positive or negative. The system achieved an accuracy of **88.17%**, which demonstrates the effectiveness of Naive Bayes for text classification tasks, particularly in scenarios with a clear distinction between classes.  \n",
    "\n",
    "Despite the high accuracy, there are several ways to further improve the model's performance:  \n",
    "\n",
    "1. **Feature Engineering**:  \n",
    "   - Include **n-grams** (e.g., bigrams and trigrams) to capture context and relationships between words.  \n",
    "   - Use **TF-IDF** (Term Frequency-Inverse Document Frequency) instead of simple word counts to weigh important terms more effectively.  \n",
    "\n",
    "2. **Advanced Preprocessing**:  \n",
    "   - Incorporate **lemmatization** instead of stemming for more meaningful root words.  \n",
    "   - Address misspelled words or slang using spell-checking and normalization techniques.  \n",
    "\n",
    "3. **Larger Vocabulary**:  \n",
    "   - Train the model on a more extensive vocabulary by using a larger or more diverse dataset.  \n",
    "\n",
    "4. **Data Augmentation**:  \n",
    "   - Use techniques like synonym replacement or back-translation to artificially expand the dataset and improve generalization.  \n",
    "\n",
    "5. **Other Classifiers**:  \n",
    "   - Experiment with other machine learning algorithms like Logistic Regression or Support Vector Machines (SVM).  \n",
    "   - Explore deep learning models like LSTMs or Transformers for better context understanding.  \n",
    "\n",
    "This project serves as a strong foundation for understanding the basics of sentiment analysis and the Naive Bayes algorithm. By incorporating the suggested improvements, the model can achieve higher accuracy and better adapt to real-world scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
